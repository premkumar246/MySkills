# Machine Learning 

## 01. Supervised Learning 

### 1.1. Regression
#### Linear Regression
#### Polynomial Regression 
#### Ridge Regression
#### Lasso Regression
#### Elastic Net
#### Support Vector Regression (SVR)
#### Decision Trees for Regression
#### Random Forests for Regression
#### Gradient Boosting for Regression (e.g., XGBoost, LightGBM, CatBoost)

### 1.2. Classification
#### Logistic Regression
#### k-Nearest Neighbors (k-NN)
#### Support Vector Machines (SVM)
#### Decision Trees for Classification
#### Random Forests for Classification
#### Gradient Boosting for Classification (e.g., XGBoost, LightGBM, CatBoost)
#### Naive Bayes (Gaussian, Multinomial, Bernoulli)
#### Neural Networks (Feedforward, Convolutional, Recurrent)
#### Ensemble Methods (Bagging, Boosting, Stacking)

## 02. Unsupervisd Learning
### 2.1. Clustering 
#### k-Means Clustering
#### Hierarchical Clustering
#### DBSCAN (Density-Based Spatial Clustering of Applications with Noise)
#### Mean Shift
#### Gaussian Mixture Models (GMM)

### 2.2. Dimensionality Reduction
#### Principal Component Analysis (PCA)
#### Linear Discriminant Analysis (LDA)
#### t-Distributed Stochastic Neighbor Embedding (t-SNE)
#### Independent Component Analysis (ICA)
#### Factor Analysis
#### UMAP (Uniform Manifold Approximation and Projection)

### 2.3. Association 
#### Apriori Algorithm
#### Eclat Algorithm
#### FP-Growth (Frequent Pattern Growth)

### 2.4. Anomaly Detection 
#### Isolation Forest
#### One-Class SVM
#### Local Outlier Factor (LOF)
#### Autoencoders for Anomaly Detection

## 03. Semi-supervised Learning 
#### Self-Training
#### Co-Training
#### Multi-View Learning
#### Generative Adversarial Networks (GANs) for Semi-Supervised Learning




